\section{Data}\label{sec:data}
In this section we will describe the data we used to test the separation routines.
First we will present the instances available from the literature and then random instances we created.
Every instance will get an abbreviation for later reference that we will denote by “Instance name (\texttt{abbreviation})”.
Furthermore, some of the instances can be parametrized in some way, \eg, the size of the instance, or the parameters of a random distribution.
We will include such parametrization parameters in the abbreviation.

\subsection{Real-world data}
\subsubsection{Cetacea (\texttt{cetacea}) and Wild Cats (\texttt{cats})}\label{subsubsec:data_cetacea}
These instances arose from the classification of cetacea (whales, porpoises, and dolphins) and wild cats, respectively.
We copied them from \cite{grotschelCuttingPlaneAlgorithm1989}, in order to sanity-check our implementation,
and consider them as the easiest among our test data, because integral solution were already found after three iterations by just separating triangle inequalities.

\subsubsection{Organoid Clustering (\texttt{organoid\_[size]\_[difficulty]})}
In \cite{presbergerSegmentationClusteringOrganoids2023}, J.\,Presberger studied clustering of organoids in light microscopy images, where \CP showed up naturally.
The findings of this work are yet to be published in a separate paper, but we obtained one instance after private communication.
It consists of pairwise similarities of 160 organoids measured in an interval $(0,1)$, where values of $1$ (resp.\ $0$) correspond to high (resp.\ low) similarity.
A challenge in modelling this similarity between different organoids is mapping one organoids components to those of another organoid.
Their solution resulted in an asymmetric cost matrix $C$, \ie for two organoids $i, j$ the similarity value $C_{i,j}$ would not necessarily equal $C_{j,i}$.
For creating a \CP instance from this cost matrix we chose $w_{ij} = \func{min}(C_{i,j}, C_{j,i})$.
Note that these are maximization problems instead of the usual minimization.

Since all entries in $C$ are positive values, the resulting \CP instance would be trivial.
We therefore offset the values by subtracting a constant value $b$ from every one.
J.\,Presberger informed us that $b \in \left\{ 0.500, 0.555, 0.585 \right\}$ were adequate values for such an offset and provided us with the optimal values for each of the corresponding instances.
The running time of the ILP solver increases drastically as the offset approaches $b = 0.500$, which is to be expected, as this creates the most ‘conflicts’.
However, we have also been informed that the obtained clusterings tend to be more meaningful for $b$’s in the vicinity of $0.500$.
We decided to call the instance \texttt{hard} ($b= 0.500$), \texttt{medium} ($b = 0.555$) and \texttt{soft} ($b = 0.585$), respectively.
For numerical reasons, we scaled all weights by multiplying them by $\num{e3}$.
Furtermore, we have created subsets of these instances by only considering the first \texttt{size} organoids, \ie the upper left $\texttt{size} \times \texttt{size}$ submatrix of $C$.

\subsubsection{Modularity Clustering}
We tested our algorithm on six instances described in \cite{kappesComparativeStudyModern2015} in the context of clustering with respect to modularity.
We refer to \cite{brandesModularityClustering2008} for the origin of three of these instances; the origin of the others is unknown to us.
Their names are in order of the instance size \texttt{football} (115), \texttt{adjnoun} (112), \texttt{polbooks} (105), \texttt{lesmis} (77), \texttt{dolphins} (62) and \texttt{karate} (34).
\cite{kappesComparativeStudyModern2015} have computed optimal solutions for all of them except \texttt{adjnoun}.
Note that these are originally instances of the \MC problem.
An optimal objective value of the corresponding \CP instance is obtained as follows:
Let $K_{n} = (V_{n}, E_{n})$ be a complete graph and let $w \in \R^{E_{n}}$ be a cost vector.
For all edges $e \in E_{n}$ we denote by $x_{e}$ the corresponding variable in the \CP instance defined by $K_{n}$ and $w$, and by $y_{e}$ the corresponding variable in the \MC problem instance defined by $K_{n}$ and $w$.
The objective values of these problems are connected as follows:
\[
	\sum_{e \in E} x_{e} w_{e} = \sum_{e \in E}^{} (1-y_{e}) w_{e} = \sum_{e \in E}^{} w_{e} - \sum_{e \in E}^{} y_{e} w_{e}
\]
This allowes us to compare the results of our algorithm to the best known solution of the \texttt{adjnoun} instance and to the optimal solutions of the other instances. 
Again, for numerical reasons, we scaled all these instances by multiplying edge weights by $\num{e3}$.

\subsection{Random Data}\label{subsec:random_data}
We also created \CP instances randomly and for different sizes.
We will abbreviate all random instances starting with an \texttt{r\_} followed by the name and size of the instance,
\eg, \texttt{r\_normal\_150\_[further parameters]} is an instance of $K_{150}$ with edge weights sampled from a normal distribution with \texttt{further parameters}.

\subsubsection{Binary Data (\texttt{r\_binary\_[size])}}\label{subsubsec:data_random_binary}
In these instances the edge weights are sampled from the set $\left\{ -1, 1 \right\}$ with equal probability.
We created them with the intetion of being ‘hard’ to solve,
because choosing between rewarding and penalizing edges randomly with the same numeric incentive should create many ‘conflicts’.

\subsubsection{Uniform Distribution (\texttt{r\_uniform\_[size]\_[lb]\_[ub]})}\label{subsubsec:data_random_uniform}
In this instance type the edge weights are sampled from a continuous uniform distribution with interval boundaries \texttt{lb} below and \texttt{ub} above and with $\text{\texttt{lb}} < \text{\texttt{ub}}$.
We suspected these instances to be easier to solve than the binary data, especially when using asymetric boundaries, \eg \texttt{r\_uniform\_[size]\_-10\_+100}, because in an instance like this, ‘conflicts’ are often already solved by the numeric value of the ‘conflicting’ edges.

\subsubsection{Normal Distribution (\texttt{r\_normal\_[size]\_[mu]\_[sigma]})}\label{subsubsec:data_random_normal}
Real-world data can often be modeled using normal distributions, which raises the question how applicable the separation routines are in practice.
The edge weights of these instances are sampled from a normal distribution with mean \texttt{mu} and standard deviation \texttt{sigma}.
We expected that a negative mean would result in ‘easier’ instances, as some subset of the negatively weighted edges might already form ‘isolated’ cliques, \ie vertices connected by negatively weighted edges to each other and by positively weighted edges to all other vertices.
As discussed earlier, these would always end up in the same block of the partition induced by an optimal clique partitioning.
This argument is ‘symmetric’ for a positive mean, where the positively weighted ‘isolated’ cliques would always end up in different blocks of the partition instead of the same block.
For this reason we could restrict our attention to positive means.
Conversely, we expected little to no influence of the standard deviation to the ‘hardness’ of the instances where $\texttt{mu} = 0$.
For instances with $\texttt{mu} \neq 0$, a narrow standard deviation might create trivial instances, where all weihgts have the same sign.
We have created instances for $(\text{\texttt{mu}}, \text{\texttt{sigma}}) \in \left\{ 0, 0.5, 2 \right\} \times \left\{ 0.5, 1, 2 \right\}$.
